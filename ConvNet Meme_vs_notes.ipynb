{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvNet",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAT0oTIl7EhpvD1CRw1Ta8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kshitij-Ambilduke/Meme-vs-Notes/blob/master/ConvNet%20Meme_vs_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWyFPNLexHq9",
        "colab_type": "text"
      },
      "source": [
        "# **Memes vs Notes Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azRaSb0LRm3f",
        "colab_type": "text"
      },
      "source": [
        "First we mount the drive containing the dataset so that we can read data from the drive and transform it as we want to.\n",
        "To do this we run the below code block and copy the password generated for us in the link provided by logging in and pasting it here.\n",
        "\n",
        "\n",
        "```python\n",
        "drive.mount('/content/drive') can be replaced by drive.mount('/content/gdrive)\n",
        "``` \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWmifWm76Vx5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "b154cbd9-7816-40bf-a0b7-42e1a5a2aa01"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCHt-VdmTDWU",
        "colab_type": "text"
      },
      "source": [
        "After having access to all the images in the dataset, we go ahead and import some important packages that will help us in reading the dataset, preprocessing and using the preprocessed data to build our model that differenciates between memes and notes.\n",
        "```python\n",
        "import os # To loacte the data and making list of all the data in a particular folder\n",
        "import cv2 # For reading the images, resizing them and converting them to a desired colour space\n",
        "import random # For Shuffling the data\n",
        "import torch # For Neural Networks\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DADlo3x96Hed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOTnm3noVhJ1",
        "colab_type": "text"
      },
      "source": [
        "Defining path for memes as ```meme_path``` and the path for notes as ```note_path```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jd30GthEV9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meme_path = \"/content/drive/My Drive/dataset stage 0 /meme\"\n",
        "note_path = \"/content/drive/My Drive/dataset stage 0 /note\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT6Jt-3iV4tc",
        "colab_type": "text"
      },
      "source": [
        "Defining a pseudo-dataloader as ```dataloader``` so as to load the data from the path that is passed as an argument ```folder``` and the label to be given to this  folder is passed through the argument ```label```.\n",
        "This function can be fine tuned so as to the desired form of the images from the folder.\n",
        "Note that all the images from the same folder are labeled with the 2nd argument passed in the function. \n",
        "Here we are using the size ```(100 x 100)``` and the grayscale colour.\n",
        "The function returns 2 lists, 1st the resized images and 2nd the labels for these images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQFkaKQJFGhg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dataloader(folder,label):\n",
        "    images=[]\n",
        "    labels=[]\n",
        "    for file in os.listdir(folder):\n",
        "        img = cv2.imread(os.path.join(folder,file),0)\n",
        "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = cv2.resize(img, (100,100))\n",
        "        images.append(img)\n",
        "        labels.append(label)\n",
        "    return images, labels"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AwDnqrBYfrT",
        "colab_type": "text"
      },
      "source": [
        "Defining variables for containing the memes and the labels for meme as ```meme``` and ```memelabel``` respectively. Similarly defining variables for notes and label for notes as ```note``` and ```notelabel``` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn91UjU3FIu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meme, memelabel = dataloader(meme_path, 1)\n",
        "note, notelabel = dataloader(note_path, 0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPiH6eSIZD1r",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Converting all the lists into numpy arrays and reshaping them in the format ```(number of images x channels in the image x height of the image x width of the image)```. For a RGB image the number of channels will be 3 and for a RGB image, the number of channel will be 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHo4M31CFLVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meme = np.array(meme)\n",
        "memelabel = np.array(memelabel)\n",
        "#print(meme.__len__())\n",
        "meme = meme.reshape(800,1,100,100)\n",
        "note = np.array(note)\n",
        "note = note.reshape(800,1,100,100)\n",
        "notelabel = np.array(notelabel)\n",
        "#print(len(notelabel))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a40s32GbOHi",
        "colab_type": "text"
      },
      "source": [
        "Zipping together the images and labels so as to Shuffle them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjDqJlebFXFE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "memezip = list(zip(meme, memelabel))\n",
        "notezip = list(zip(note, notelabel))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_rvUCfnbaAv",
        "colab_type": "text"
      },
      "source": [
        "Concatenating the two lists in the variable ```toshuffle``` then shuffling the list containing the zips of both the labels using  ```random.shuffle()``` then unzipping the shuffled list of zips into two variables ```xtest``` and ```ytest``` where ```xtest``` is the list containing the shuffled images and ```ytest``` is the list containing the labels for these images.\n",
        "In the same way ```xtrain```,```ytrain```,```xval```,```yval``` is created.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Q377WQFYOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toshuffle = []\n",
        "toshuffle = memezip[0:750] + notezip[0:750]\n",
        "random.shuffle(toshuffle)\n",
        "xtrain, ytrain = list(zip(*toshuffle))\n",
        "\n",
        "toshuffle = []\n",
        "toshuffle = memezip[750:775] + notezip[750:775]\n",
        "random.shuffle(toshuffle)\n",
        "xtest, ytest = list(zip(*toshuffle))\n",
        "\n",
        "toshuffle = []\n",
        "toshuffle = memezip[775:] + notezip[775:]\n",
        "random.shuffle(toshuffle)\n",
        "xval, yval = list(zip(*toshuffle))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbhCyoBig-VF",
        "colab_type": "text"
      },
      "source": [
        "Now that we have our data all sorted out, its time to convert our data into tensor so as to use it with pytorch module.\n",
        "While calculating the loss, the labels of the data are expected to be of ```Long``` datatype so we convert the labels into ```long``` datatype."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nfSBtKijoKl",
        "colab_type": "text"
      },
      "source": [
        "We have also segregated the data into training, validation and testing data.\n",
        "Out of the **1600** images available to us in the dataset, we chose **1500** images for training, 50 for validation and **50** for testing. This gives us a training set of **93.75%** of the dataset and the validation and testing set bith **3.125%** of the total dataset. Here we have adopted a very primitive technique of normalization that being dividing all the pixels of the image by 255 thereby bringing each of the pixels' value between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtsW8-IZFkoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain = torch.Tensor(xtrain)\n",
        "xtest = torch.Tensor(xtest)\n",
        "xval = torch.Tensor(xval)\n",
        "ytrain = torch.Tensor(ytrain).long()\n",
        "ytest = torch.Tensor(ytest).long()\n",
        "yval = torch.Tensor(yval).long()\n",
        "xtrain/=255.0\n",
        "xtest/=255.0\n",
        "xval /=255.0"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Yr46EslqfS",
        "colab_type": "text"
      },
      "source": [
        "Now we start to define our neural network. We start by creating a class ```ConvNet``` which inherit from the parent class of pytorch ```nn.Module```. The attributes of this class are the layers of the neural network are the layers of our convolutional neural network. We use ```nn.Sequntial``` for bunching a couple of layes together.\n",
        "The Architechture of the neural network is as follows:\n",
        "\n",
        "\n",
        "1.   Convolutional layer taking 1 channel and giving 16 channels with a kernel size of ```5x5``` followed by a ReLU activation and then a Max Pool layer with a ```kernel_size = 2``` and ```stride = 2```\n",
        "2.   Convolutional layer taking 16 channel and giving 32 channels with a kernel size of ```5x5``` followed by a ReLU activation and then a Max Pool layer with a ```kernel_size = 2``` and ```stride = 2```\n",
        "3.   Convolutional layer taking 32 channel and giving 48 channels with a kernel size of ```5x5``` followed by a ReLU activation and then a Max Pool layer with a ```kernel_size = 2``` and ```stride = 2```\n",
        "4.    Couple of Linear layer with ```input_features = 2888``` i.e. the flattened output of the convolutional layers, then hidden layer containing ```500``` neurons and the output layer containing ```2``` neurons which depict the binary nature of our classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3BFTLic6iBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.l1 = nn.Sequential(nn.Conv2d(1,16,5),\n",
        "                       nn.ReLU(),\n",
        "                       nn.MaxPool2d(2,2))\n",
        "    \n",
        "    self.l2 = nn.Sequential(nn.Conv2d(16,32,3),\n",
        "                       nn.ReLU(),\n",
        "                       nn.MaxPool2d(2,2))\n",
        "    \n",
        "    self.l3 = nn.Sequential(nn.Conv2d(32,48,5),\n",
        "                       nn.ReLU(),\n",
        "                       nn.MaxPool2d(2,2))\n",
        "\n",
        "    self.l4 = nn.Sequential(nn.Linear(48*9*9,500),\n",
        "                       nn.Linear(500,2))\n",
        "    \n",
        "  def forward(self,x):\n",
        "\n",
        "    x = self.l1(x)\n",
        "    x = self.l2(x)\n",
        "    x = self.l3(x)\n",
        "    x = x.flatten(1)\n",
        "    x = self.l4(x)\n",
        "    return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhtva7JbuRGF",
        "colab_type": "text"
      },
      "source": [
        "Now we create an instance of the class we just created and print out that instance just to make sure everything is just as we wanted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg5XBNHsESoq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "047c0e69-c7cb-43f8-b733-0ead43254084"
      },
      "source": [
        "net = ConvNet()\n",
        "print(net)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConvNet(\n",
            "  (l1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (l2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (l3): Sequential(\n",
            "    (0): Conv2d(32, 48, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (l4): Sequential(\n",
            "    (0): Linear(in_features=3888, out_features=500, bias=True)\n",
            "    (1): Linear(in_features=500, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waJhL1F3uqPC",
        "colab_type": "text"
      },
      "source": [
        "Now it's time we define our optimizer and loss function. Pytorch provied optimizer in the ```torch.optim``` package so we define our optimizer as ```torch.optim.Adam``` optimizer and the loss function as ```nn.CrossEntropyLoss```. This loss function combines the ```softmax``` and the ```NLLloss``` so we need not define the softmax layer at the end of our network. The optimizer requires 2 arguments, 1st the parameters on which the optimization should be performed and 2nd the learning rate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1LdVFYLNpUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(),lr=0.0001)\n",
        "lossfn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Zn4WXTv_7u",
        "colab_type": "text"
      },
      "source": [
        "Finally with all the data ready and the loss and optimiser defined, we can now build our accuracy function so as to track the training, testing and validation accuracy.\n",
        "This function takes 2 arguments ```label``` and ```pred```.\n",
        "The argument ```label``` is the actual label matrix that we made while we made the dataset and ```pred``` is a matrix output by the network with contains the confidence score of what the network thinks should be the prediction of the data. We use ```torch.argmax``` to get the index of the highest prediction. The argument ```dim=1``` specifies that for a particular example, the confidence score is distributed along the horizontal. Then by comparing the values in the 2 matrices, we can get the accuracy easily. \n",
        "> While calculating the accuracy, we need to apply the softmax layer so as to get the probability distribution of of the confidence score of the model. This is done because we haven't applied Softmax layer to the output but while loss calculation, due to usage of ```nn.CrossEntropyLoss``` we dont need to separately use Softmax .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pM5whC3fOErr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(label, pred):\n",
        "  pred = F.softmax(pred, dim=1)\n",
        "  count =0\n",
        "  pred = torch.argmax(pred, dim=1, keepdims=True)\n",
        "  pred = pred.reshape(pred.shape[0])\n",
        "  for i in range(len(label)):\n",
        "    if label[i]==pred[i]:\n",
        "      count+=1\n",
        "  return 100*count/label.shape[0]\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acvDaAv-T2VJ",
        "colab_type": "text"
      },
      "source": [
        "Now we define our main loop which will contain everything and it will do all the calculations necessary for our network to learn. I have chosen a mini-batch of size 50 and 3 epochs. The training accuracy, testing accuracy and validation accuracy is stored in the lists ```trainacc```,```testacc``` and ```valacc``` respectively. Similarly, the training and validation loss is stored in ```trainloss``` and ```valloss``` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OsVixXBLidy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2af8ebde-d23f-43bd-ba71-cc31da124389"
      },
      "source": [
        "trainacc = []\n",
        "testacc = []\n",
        "trainloss = []\n",
        "testloss = []\n",
        "valacc = []\n",
        "valloss = []\n",
        "\n",
        "for i in range(10):\n",
        "    k=0\n",
        "    for j in range(30): \n",
        "      \n",
        "      output = net.forward(xtrain[k:k+50])\n",
        "      loss = lossfn(output, ytrain[k:k+50])\n",
        "      trainloss.append(loss)\n",
        "        \n",
        "      k+=50\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "      accu = accuracy(ytrain, net.forward(xtrain))\n",
        "      vloss = lossfn(net.forward(xval),yval)\n",
        "      tacc = accuracy(ytest, net.forward(xtest))\n",
        "      vacc = accuracy(yval,net.forward(xval))\n",
        "\n",
        "    \n",
        "    print(f\"training loss:{loss}\")\n",
        "    print(f\"validation loss:{vloss}\")\n",
        "    valloss.append(vloss.item())\n",
        "    print(f\"training accuracy:{accu}\")\n",
        "    trainacc.append(accu)\n",
        "    print(f\"validation accuracy:{vacc}\")\n",
        "    valacc.append(vacc)\n",
        "    print(f\"testing accuracy:{tacc}\")\n",
        "    testacc.append(tacc)\n",
        "    print()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training loss:0.21328027546405792\n",
            "validation loss:0.1966356635093689\n",
            "training accuracy:95.86666666666666\n",
            "validation accuracy:92.0\n",
            "testing accuracy:92.0\n",
            "\n",
            "training loss:0.1951848417520523\n",
            "validation loss:0.15445011854171753\n",
            "training accuracy:95.86666666666666\n",
            "validation accuracy:92.0\n",
            "testing accuracy:90.0\n",
            "\n",
            "training loss:0.19180795550346375\n",
            "validation loss:0.1485862284898758\n",
            "training accuracy:96.8\n",
            "validation accuracy:92.0\n",
            "testing accuracy:92.0\n",
            "\n",
            "training loss:0.19153982400894165\n",
            "validation loss:0.1517157256603241\n",
            "training accuracy:97.33333333333333\n",
            "validation accuracy:92.0\n",
            "testing accuracy:94.0\n",
            "\n",
            "training loss:0.18027454614639282\n",
            "validation loss:0.1502465307712555\n",
            "training accuracy:97.53333333333333\n",
            "validation accuracy:94.0\n",
            "testing accuracy:94.0\n",
            "\n",
            "training loss:0.1566782146692276\n",
            "validation loss:0.1392943263053894\n",
            "training accuracy:97.53333333333333\n",
            "validation accuracy:94.0\n",
            "testing accuracy:96.0\n",
            "\n",
            "training loss:0.1245923638343811\n",
            "validation loss:0.1189323216676712\n",
            "training accuracy:98.06666666666666\n",
            "validation accuracy:94.0\n",
            "testing accuracy:96.0\n",
            "\n",
            "training loss:0.0962042361497879\n",
            "validation loss:0.09684586524963379\n",
            "training accuracy:98.26666666666667\n",
            "validation accuracy:96.0\n",
            "testing accuracy:98.0\n",
            "\n",
            "training loss:0.08572543412446976\n",
            "validation loss:0.09080540388822556\n",
            "training accuracy:98.46666666666667\n",
            "validation accuracy:96.0\n",
            "testing accuracy:98.0\n",
            "\n",
            "training loss:0.08701584488153458\n",
            "validation loss:0.09862508624792099\n",
            "training accuracy:98.4\n",
            "validation accuracy:96.0\n",
            "testing accuracy:98.0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIS8GNjktbgo",
        "colab_type": "text"
      },
      "source": [
        "## **Note:**\n",
        "> If you are re-running the program start from the point where we made ```xtrain```,```ytrain``` ....\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgotRT7bqC6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8606ed70-ce4d-4ff0-aa40-07b3ad88342a"
      },
      "source": [
        "print(xtrain.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 1, 100, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hyuuGRONm8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "def plot(data):\n",
        "  plt.plot(data)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQKUaeCdTVSr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "984c20e4-8200-446b-f430-10f59c106323"
      },
      "source": [
        "plot(testacc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbrEL5EK74fU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}